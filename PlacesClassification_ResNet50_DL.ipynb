{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports for building a deep learning model using PyTorch, Torchvision, and other essential libraries.\n",
    "\n",
    "- PyTorch: Provides deep learning functionality, including neural networks, optimization, and custom datasets.\n",
    "- Torchvision: Contains utilities for vision-based tasks, including datasets and image transformations.\n",
    "- Image handling and visualization: Libraries for handling and displaying images, including OpenCV.\n",
    "- Data manipulation: Libraries for handling data, file processing, and randomization.\n",
    "\"\"\"\n",
    "\n",
    "# PyTorch core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, SubsetRandomSampler  # Dataset for custom dataset creation\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# CUDA\n",
    "from torch.cuda import amp\n",
    "\n",
    "# Torchvision imports for vision-based tasks\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "# Image handling and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2  # OpenCV for image processing\n",
    "from tqdm import tqdm # For progress bars \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Data manipulation and file handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter  # Counting utility for analyzing data\n",
    "import glob  # File path handling\n",
    "import os  # Operating system interface for directory management\n",
    "\n",
    "# Randomization\n",
    "from random import shuffle, seed  # Random shuffling and seeding for reproducibility\n",
    "\n",
    "# Optimization\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57bd1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b03e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Free unused memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa23863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f50715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "TRAIN_DIR = './data/data_256'\n",
    "# TEST_DIR = './data/test_256'\n",
    "# VAL_DIR = './data/val_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac6172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_dir(directory):\n",
    "    '''\n",
    "    Counts the number of files in each subdirectory of the given directory,\n",
    "    returning a DataFrame with folder names, subfolder names, and their image counts.\n",
    "\n",
    "    Type 'quit' to exit, or 'default' to display all folders.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the main directory containing subfolders.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): DataFrame containing folder names, subfolder names, and image counts.\n",
    "    '''\n",
    "    data = []\n",
    "\n",
    "    # Iterate through subdirectories (folders)\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "\n",
    "        # Check if it's a directory (i.e., a folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Iterate through subdirectories (subfolders)\n",
    "            for subdir in os.listdir(folder_path):\n",
    "                subdir_path = os.path.join(folder_path, subdir)\n",
    "\n",
    "                # Check if it's a directory (i.e., a subfolder)\n",
    "                if os.path.isdir(subdir_path):\n",
    "                    # Count number of files in the subdirectory\n",
    "                    file_count = len(os.listdir(subdir_path))\n",
    "                    data.append({'Folder': folder, 'Subfolder': subdir, 'Image Count': file_count})\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    user_input = input(\"Enter your choice: \").strip().lower()\n",
    "\n",
    "    if user_input == 'quit':\n",
    "        print(\"Exiting the program.\")\n",
    "        return None  # or you can raise an exception or return a specific value if needed\n",
    "    elif user_input == 'default':\n",
    "        pd.set_option('display.max_rows', None)  # Show all rows in DataFrame\n",
    "        return df\n",
    "    elif user_input.isalpha() and len(user_input) == 1:\n",
    "        # Display the selected folder's subfolders\n",
    "        filtered_df = df[df['Folder'].str.lower() == user_input]  # Filter by folder\n",
    "        if not filtered_df.empty:\n",
    "            pd.set_option('display.max_rows', None)  # Show all rows in DataFrame\n",
    "            return filtered_df\n",
    "        else:\n",
    "            print(f\"No subfolders found for folder: {user_input}\")\n",
    "            return None  # or handle this case as needed\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter a valid folder letter or 'quit'.\")\n",
    "        return None  # or handle this case as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e2635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the program.\n"
     ]
    }
   ],
   "source": [
    "# Count files in train and test directories\n",
    "train_class_counts = count_files_in_dir(TRAIN_DIR)\n",
    "train_class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c01faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_label_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_label_list (list of tuples): List where each tuple is (image_path, label).\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.image_label_list = image_label_list\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_label_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_label_list[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "def load_preprocess_resnet18(train_dir, batch_size=64, val_size=0.2, test_size=0.1, num_classes=5):\n",
    "    '''\n",
    "    Load and preprocess the data by randomly selecting a specified number of classes\n",
    "    from the full dataset and splitting it into training, validation, and test sets.\n",
    "    Labels are remapped to ensure they are contiguous (0, 1, 2, ..., num_classes-1).\n",
    "\n",
    "    Args:\n",
    "        train_dir (str): Path to the training data directory.\n",
    "        batch_size (int): The batch size for the DataLoader.\n",
    "        val_size (float): Proportion of the dataset to be used as validation data.\n",
    "        test_size (float): Proportion of the dataset to be used as test data.\n",
    "        num_classes (int): Number of random classes to select from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: DataLoader for training, validation, and test datasets, and list of selected class names.\n",
    "    '''\n",
    "    \n",
    "    # Define the transformations for ResNet50\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "    ])\n",
    "    \n",
    "    # Step 1: Collect all class names and image paths\n",
    "    class_names = []\n",
    "    class_to_images = {}\n",
    "    \n",
    "    for letter in os.listdir(train_dir):\n",
    "        letter_dir = os.path.join(train_dir, letter)\n",
    "        if os.path.isdir(letter_dir):\n",
    "            for class_name in os.listdir(letter_dir):\n",
    "                class_dir = os.path.join(letter_dir, class_name)\n",
    "                if os.path.isdir(class_dir):\n",
    "                    class_names.append(class_name)\n",
    "                    image_files = [\n",
    "                        os.path.join(class_dir, img) for img in os.listdir(class_dir)\n",
    "                        if img.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "                    ]\n",
    "                    class_to_images[class_name] = image_files\n",
    "    \n",
    "    print(f\"Total classes found: {len(class_names)}\")\n",
    "    \n",
    "    # Step 2: Randomly select num_classes\n",
    "    if num_classes > len(class_names):\n",
    "        raise ValueError(f\"num_classes={num_classes} exceeds available classes={len(class_names)}\")\n",
    "    \n",
    "    selected_classes = random.sample(class_names, num_classes)\n",
    "    print(f\"Selected classes: {selected_classes}\")\n",
    "    \n",
    "    # Step 3: Collect image paths and assign new labels\n",
    "    selected_class_to_idx = {class_name: idx for idx, class_name in enumerate(selected_classes)}\n",
    "    image_label_list = []\n",
    "    \n",
    "    for class_name in selected_classes:\n",
    "        image_paths = class_to_images[class_name]\n",
    "        for img_path in image_paths:\n",
    "            image_label_list.append((img_path, selected_class_to_idx[class_name]))\n",
    "    \n",
    "    print(f\"Number of filtered samples: {len(image_label_list)}\")\n",
    "    \n",
    "    if len(image_label_list) == 0:\n",
    "        raise ValueError(\"No images found for the selected classes.\")\n",
    "    \n",
    "    # Step 4: Split into train, val, test\n",
    "    # Extract labels for stratification\n",
    "    labels = [label for _, label in image_label_list]\n",
    "    \n",
    "    # First split: train and temp (val + test)\n",
    "    train_indices, temp_indices, train_labels, temp_labels = train_test_split(\n",
    "        list(range(len(image_label_list))),\n",
    "        labels,\n",
    "        test_size=(val_size + test_size),\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Calculate relative size for validation\n",
    "    relative_val_size = val_size / (val_size + test_size)\n",
    "    \n",
    "    # Second split: val and test\n",
    "    val_indices, test_indices, _, _ = train_test_split(\n",
    "        temp_indices,\n",
    "        [labels[i] for i in temp_indices],\n",
    "        test_size=(test_size / (val_size + test_size)),\n",
    "        random_state=42,\n",
    "        stratify=[labels[i] for i in temp_indices]\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(train_indices)}, Validation samples: {len(val_indices)}, Test samples: {len(test_indices)}\")\n",
    "    \n",
    "    # Create subset datasets\n",
    "    train_subset = Subset(CustomDataset(image_label_list, transform=transform), train_indices)\n",
    "    val_subset = Subset(CustomDataset(image_label_list, transform=transform), val_indices)\n",
    "    test_subset = Subset(CustomDataset(image_label_list, transform=transform), test_indices)\n",
    "    \n",
    "    # Create DataLoaders with adjusted parameters\n",
    "    train_loader = DataLoader(\n",
    "        train_subset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=0,          # Changed from 4 to 0\n",
    "        pin_memory=False        # Changed from True to False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0,          # Changed from 4 to 0\n",
    "        pin_memory=False        # Changed from True to False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_subset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0,          # Changed from 4 to 0\n",
    "        pin_memory=False        # Changed from True to False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, selected_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6496c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes found: 274\n",
      "Selected classes: ['campus', 'archaelogical_excavation', 'junkyard', 'hospital', 'gift_shop']\n",
      "Number of filtered samples: 25000\n",
      "Train samples: 17499, Validation samples: 5000, Test samples: 2501\n",
      "\n",
      "Train Loader:\n",
      "Batch 1:\n",
      "Images shape: torch.Size([64, 3, 224, 224])\n",
      "Labels: tensor([1, 0, 2, 1, 3, 4, 0, 0, 2, 1, 3, 3, 0, 4, 1, 3, 3, 2, 2, 0, 2, 1, 0, 1,\n",
      "        4, 4, 2, 2, 3, 3, 1, 3, 0, 0, 0, 2, 1, 4, 3, 4, 4, 4, 2, 1, 4, 1, 3, 1,\n",
      "        3, 2, 3, 2, 1, 3, 4, 1, 0, 1, 1, 0, 1, 2, 3, 3])\n",
      "\n",
      "Validation Loader:\n",
      "Batch 1:\n",
      "Images shape: torch.Size([64, 3, 224, 224])\n",
      "Labels: tensor([2, 0, 4, 1, 2, 1, 0, 3, 2, 0, 4, 1, 4, 0, 1, 1, 4, 2, 2, 2, 4, 2, 3, 2,\n",
      "        2, 4, 2, 3, 1, 0, 1, 1, 4, 3, 2, 0, 3, 4, 2, 1, 0, 4, 4, 2, 2, 2, 1, 2,\n",
      "        1, 1, 4, 2, 3, 1, 3, 2, 4, 2, 3, 4, 2, 0, 2, 3])\n",
      "\n",
      "Test Loader:\n",
      "Batch 1:\n",
      "Images shape: torch.Size([64, 3, 224, 224])\n",
      "Labels: tensor([4, 0, 0, 2, 4, 0, 3, 2, 1, 1, 0, 3, 0, 0, 0, 3, 3, 2, 3, 2, 2, 3, 2, 3,\n",
      "        2, 2, 3, 0, 2, 3, 0, 0, 3, 1, 2, 2, 4, 3, 3, 2, 0, 1, 1, 4, 1, 3, 3, 2,\n",
      "        0, 3, 0, 4, 0, 1, 3, 2, 0, 4, 0, 2, 3, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "def print_loader_samples(loader, loader_name, num_batches=1):\n",
    "    print(f\"\\n{loader_name}:\")\n",
    "    for i, data in enumerate(loader):\n",
    "        print(f\"Batch {i+1}:\")\n",
    "        if isinstance(data, tuple) or isinstance(data, list):\n",
    "            images, labels = data\n",
    "            print(f\"Images shape: {images.shape}\")\n",
    "            print(f\"Labels: {labels}\")\n",
    "        else:\n",
    "            images = data\n",
    "            print(f\"Images shape: {images.shape}\")\n",
    "            print(\"Labels: None\")\n",
    "        if i + 1 == num_batches:\n",
    "            break\n",
    "    \n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Load data loaders\n",
    "train_loader, val_loader, test_loader, class_names = load_preprocess_resnet18(\n",
    "    train_dir=TRAIN_DIR,\n",
    "    batch_size=64,\n",
    "    val_size=0.2,\n",
    "    test_size=0.1,\n",
    "    num_classes=5\n",
    ")\n",
    "\n",
    "# Print samples from each loader\n",
    "print_loader_samples(train_loader, \"Train Loader\", num_batches=1)\n",
    "print_loader_samples(val_loader, \"Validation Loader\", num_batches=1)\n",
    "print_loader_samples(test_loader, \"Test Loader\", num_batches=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4958a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=5, no=512, kernel_size=3, freeze_resnet=True):\n",
    "        super(ResNet18_CNN, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet18 model with default weights\n",
    "        self.resnet18 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        if freeze_resnet:\n",
    "            # Freeze all ResNet18 layers\n",
    "            for param in self.resnet18.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze the last block (layer4) for fine-tuning\n",
    "        for param in self.resnet18.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Remove the original fully connected layer and the average pool\n",
    "        self.features = nn.Sequential(*list(self.resnet18.children())[:-2])  # Output: [batch, 512, 7, 7]\n",
    "        \n",
    "        # Add custom convolutional layer\n",
    "        self.conv = nn.Conv2d(in_channels=512, out_channels=no, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Add Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Add global average pooling and a fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(no, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)          # [batch, 512, 7, 7]\n",
    "        x = self.conv(x)              # [batch, no, 7, 7]\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)           # Apply Dropout\n",
    "        x = self.avgpool(x)           # [batch, no, 1, 1]\n",
    "        x = torch.flatten(x, 1)       # [batch, no]\n",
    "        x = self.fc(x)                # [batch, num_classes]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f48cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_cnn(num_epochs=20, learning_rate=0.001):\n",
    "    # Initialize the model, load it to the appropriate device\n",
    "    model = ResNet18_CNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # For visualization\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move to GPU if available\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += (preds == targets).sum().item()\n",
    "            total_train += targets.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)  # Move to GPU if available\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Accumulate loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == targets).sum().item()\n",
    "                total_val += targets.size(0)\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move to GPU if available\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Accumulate loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_test += (preds == targets).sum().item()\n",
    "            total_test += targets.size(0)\n",
    "\n",
    "    # Calculate final test loss and accuracy\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = correct_test / total_test\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Visualization of loss and accuracy over epochs\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(range(1, num_epochs+1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12bfcb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yashh\\Downloads\\CV_Places365\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1159: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "----------\n",
      "  Batch 10/274 - Loss: 0.6172\n",
      "  Batch 20/274 - Loss: 0.7858\n",
      "  Batch 30/274 - Loss: 0.3458\n",
      "  Batch 40/274 - Loss: 0.4661\n",
      "  Batch 50/274 - Loss: 0.4459\n",
      "  Batch 60/274 - Loss: 0.4183\n",
      "  Batch 70/274 - Loss: 0.4753\n",
      "  Batch 80/274 - Loss: 0.5736\n",
      "  Batch 90/274 - Loss: 0.3200\n",
      "  Batch 100/274 - Loss: 0.4187\n",
      "  Batch 110/274 - Loss: 0.4120\n",
      "  Batch 120/274 - Loss: 0.3380\n",
      "  Batch 130/274 - Loss: 0.2474\n",
      "  Batch 140/274 - Loss: 0.4474\n",
      "  Batch 150/274 - Loss: 0.3455\n",
      "  Batch 160/274 - Loss: 0.4235\n",
      "  Batch 170/274 - Loss: 0.2942\n",
      "  Batch 180/274 - Loss: 0.2725\n",
      "  Batch 190/274 - Loss: 0.3337\n",
      "  Batch 200/274 - Loss: 0.5654\n",
      "  Batch 210/274 - Loss: 0.2622\n",
      "  Batch 220/274 - Loss: 0.2896\n",
      "  Batch 230/274 - Loss: 0.4669\n",
      "  Batch 240/274 - Loss: 0.2089\n",
      "  Batch 250/274 - Loss: 0.3220\n",
      "  Batch 260/274 - Loss: 0.6178\n",
      "  Batch 270/274 - Loss: 0.4371\n",
      "Training Loss: 0.4124, Training Accuracy: 0.8502\n",
      "Validation Loss: 0.3005, Validation Accuracy: 0.8866\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n",
      "  Batch 10/274 - Loss: 0.1604\n",
      "  Batch 20/274 - Loss: 0.3283\n",
      "  Batch 30/274 - Loss: 0.1492\n",
      "  Batch 40/274 - Loss: 0.0653\n",
      "  Batch 50/274 - Loss: 0.1543\n",
      "  Batch 60/274 - Loss: 0.1483\n",
      "  Batch 70/274 - Loss: 0.3075\n",
      "  Batch 80/274 - Loss: 0.3033\n",
      "  Batch 90/274 - Loss: 0.2389\n",
      "  Batch 100/274 - Loss: 0.1921\n",
      "  Batch 110/274 - Loss: 0.2965\n",
      "  Batch 120/274 - Loss: 0.2512\n",
      "  Batch 130/274 - Loss: 0.5011\n",
      "  Batch 140/274 - Loss: 0.1782\n",
      "  Batch 150/274 - Loss: 0.3719\n",
      "  Batch 160/274 - Loss: 0.2617\n",
      "  Batch 170/274 - Loss: 0.2453\n",
      "  Batch 180/274 - Loss: 0.2596\n",
      "  Batch 190/274 - Loss: 0.1906\n",
      "  Batch 200/274 - Loss: 0.3208\n",
      "  Batch 210/274 - Loss: 0.1945\n",
      "  Batch 220/274 - Loss: 0.1318\n",
      "  Batch 230/274 - Loss: 0.2448\n",
      "  Batch 240/274 - Loss: 0.3407\n",
      "  Batch 250/274 - Loss: 0.3678\n",
      "  Batch 260/274 - Loss: 0.2117\n",
      "  Batch 270/274 - Loss: 0.2005\n",
      "Training Loss: 0.2690, Training Accuracy: 0.8962\n",
      "Validation Loss: 0.2884, Validation Accuracy: 0.8944\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n",
      "  Batch 10/274 - Loss: 0.1858\n",
      "  Batch 20/274 - Loss: 0.1841\n",
      "  Batch 30/274 - Loss: 0.2099\n",
      "  Batch 40/274 - Loss: 0.1930\n",
      "  Batch 50/274 - Loss: 0.1381\n",
      "  Batch 60/274 - Loss: 0.2183\n",
      "  Batch 70/274 - Loss: 0.5619\n",
      "  Batch 80/274 - Loss: 0.4378\n",
      "  Batch 90/274 - Loss: 0.1647\n",
      "  Batch 100/274 - Loss: 0.1726\n",
      "  Batch 110/274 - Loss: 0.2622\n",
      "  Batch 120/274 - Loss: 0.1816\n",
      "  Batch 130/274 - Loss: 0.2197\n",
      "  Batch 140/274 - Loss: 0.1475\n",
      "  Batch 150/274 - Loss: 0.2645\n",
      "  Batch 160/274 - Loss: 0.1530\n",
      "  Batch 170/274 - Loss: 0.2355\n",
      "  Batch 180/274 - Loss: 0.1918\n",
      "  Batch 190/274 - Loss: 0.2491\n",
      "  Batch 200/274 - Loss: 0.1646\n",
      "  Batch 210/274 - Loss: 0.1668\n",
      "  Batch 220/274 - Loss: 0.2243\n",
      "  Batch 230/274 - Loss: 0.2492\n",
      "  Batch 240/274 - Loss: 0.1340\n",
      "  Batch 250/274 - Loss: 0.3942\n",
      "  Batch 260/274 - Loss: 0.1870\n",
      "  Batch 270/274 - Loss: 0.3158\n",
      "Training Loss: 0.2180, Training Accuracy: 0.9179\n",
      "Validation Loss: 0.3248, Validation Accuracy: 0.8796\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n",
      "  Batch 10/274 - Loss: 0.1802\n",
      "  Batch 20/274 - Loss: 0.1293\n",
      "  Batch 30/274 - Loss: 0.1272\n",
      "  Batch 40/274 - Loss: 0.1042\n",
      "  Batch 50/274 - Loss: 0.2095\n",
      "  Batch 60/274 - Loss: 0.1110\n",
      "  Batch 70/274 - Loss: 0.1325\n",
      "  Batch 80/274 - Loss: 0.0702\n",
      "  Batch 90/274 - Loss: 0.1495\n",
      "  Batch 100/274 - Loss: 0.4208\n",
      "  Batch 110/274 - Loss: 0.1945\n",
      "  Batch 120/274 - Loss: 0.1900\n",
      "  Batch 130/274 - Loss: 0.1316\n",
      "  Batch 140/274 - Loss: 0.2195\n",
      "  Batch 150/274 - Loss: 0.0401\n",
      "  Batch 160/274 - Loss: 0.1963\n",
      "  Batch 170/274 - Loss: 0.2584\n",
      "  Batch 180/274 - Loss: 0.1093\n",
      "  Batch 190/274 - Loss: 0.2168\n",
      "  Batch 200/274 - Loss: 0.0857\n",
      "  Batch 210/274 - Loss: 0.1312\n",
      "  Batch 220/274 - Loss: 0.1380\n",
      "  Batch 230/274 - Loss: 0.1584\n",
      "  Batch 240/274 - Loss: 0.0984\n",
      "  Batch 250/274 - Loss: 0.1528\n",
      "  Batch 260/274 - Loss: 0.1693\n",
      "  Batch 270/274 - Loss: 0.2108\n",
      "Training Loss: 0.1610, Training Accuracy: 0.9363\n",
      "Validation Loss: 0.3244, Validation Accuracy: 0.8856\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n",
      "  Batch 10/274 - Loss: 0.0785\n",
      "  Batch 20/274 - Loss: 0.0261\n",
      "  Batch 30/274 - Loss: 0.0471\n",
      "  Batch 40/274 - Loss: 0.0092\n",
      "  Batch 50/274 - Loss: 0.1397\n",
      "  Batch 60/274 - Loss: 0.1076\n",
      "  Batch 70/274 - Loss: 0.0564\n",
      "  Batch 80/274 - Loss: 0.1581\n",
      "  Batch 90/274 - Loss: 0.2412\n",
      "  Batch 100/274 - Loss: 0.0913\n",
      "  Batch 110/274 - Loss: 0.1637\n",
      "  Batch 120/274 - Loss: 0.3244\n",
      "  Batch 130/274 - Loss: 0.0817\n",
      "  Batch 140/274 - Loss: 0.0876\n",
      "  Batch 150/274 - Loss: 0.1606\n",
      "  Batch 160/274 - Loss: 0.1098\n",
      "  Batch 170/274 - Loss: 0.0406\n",
      "  Batch 180/274 - Loss: 0.0782\n",
      "  Batch 190/274 - Loss: 0.1399\n",
      "  Batch 200/274 - Loss: 0.2113\n",
      "  Batch 210/274 - Loss: 0.0858\n",
      "  Batch 220/274 - Loss: 0.0873\n",
      "  Batch 230/274 - Loss: 0.2598\n",
      "  Batch 240/274 - Loss: 0.1472\n",
      "  Batch 250/274 - Loss: 0.1359\n",
      "  Batch 260/274 - Loss: 0.1401\n",
      "  Batch 270/274 - Loss: 0.1102\n",
      "Training Loss: 0.1117, Training Accuracy: 0.9589\n",
      "Validation Loss: 0.3837, Validation Accuracy: 0.8802\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n",
      "  Batch 10/274 - Loss: 0.0664\n",
      "  Batch 20/274 - Loss: 0.0100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and visualize the model's performance\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_val_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 36\u001b[0m, in \u001b[0;36mtrain_val_cnn\u001b[1;34m(num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m     39\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and visualize the model's performance\n",
    "train_val_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
